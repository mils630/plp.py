import os
import requests
import hashlib
from urllib.parse import urlparse

def sanitize_filename(filename):
    """Ensure filename is safe for saving locally."""
    return "".join(c for c in filename if c.isalnum() or c in (' ', '.', '_')).rstrip()

def get_file_hash(filepath):
    """Return SHA256 hash of a file (to check for duplicates)."""
    hash_sha = hashlib.sha256()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_sha.update(chunk)
    return hash_sha.hexdigest()

def fetch_images(urls):
    save_dir = "Fetched_Images"
    os.makedirs(save_dir, exist_ok=True)

    downloaded_hashes = set()  # Track hashes to avoid duplicates

    for url in urls:
        url = url.strip()
        if not url:
            continue

        try:
            print(f" Connecting to: {url}")
            response = requests.get(url, stream=True, timeout=10)
            response.raise_for_status()  # Raises HTTPError for bad status

            # --- Precautions: Check important HTTP headers ---
            content_type = response.headers.get("Content-Type", "")
            if not content_type.startswith("image/"):
                print(f"⚠️ Skipped: {url} (Not an image, Content-Type={content_type})")
                continue

            # Extract filename from URL
            parsed_url = urlparse(url)
            filename = os.path.basename(parsed_url.path)
            if not filename:
                filename = "downloaded_image.jpg"

            filename = sanitize_filename(filename)
            file_path = os.path.join(save_dir, filename)

            # Save image temporarily to check hash
            with open(file_path, "wb") as file:
                for chunk in response.iter_content(1024):
                    file.write(chunk)

            # Compute hash to detect duplicates
            file_hash = get_file_hash(file_path)
            if file_hash in downloaded_hashes:
                print(f" Duplicate detected, deleting: {filename}")
                os.remove(file_path)
                continue

            downloaded_hashes.add(file_hash)
            print(f" Image saved: {file_path}")

        except requests.exceptions.RequestException as e:
            print(f" Could not fetch {url}. Reason: {e}")

if __name__ == "__main__":
    print("Ubuntu-Inspired Image Fetcher ")
    print("Enter multiple image URLs separated by commas:")
    user_input = input("URLs: ")

    urls = user_input.split(",")
    fetch_images(urls)
